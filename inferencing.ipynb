{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T11:26:47.725460Z",
     "iopub.status.busy": "2025-02-26T11:26:47.725168Z",
     "iopub.status.idle": "2025-02-26T11:26:47.943963Z",
     "shell.execute_reply": "2025-02-26T11:26:47.943283Z",
     "shell.execute_reply.started": "2025-02-26T11:26:47.725437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = \"please add your token\"  \n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:11:01.831164Z",
     "iopub.status.busy": "2025-02-26T12:11:01.830838Z",
     "iopub.status.idle": "2025-02-26T12:11:05.520466Z",
     "shell.execute_reply": "2025-02-26T12:11:05.519538Z",
     "shell.execute_reply.started": "2025-02-26T12:11:01.831134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ba61fdb7fc47e484d89770d8a79d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from /kaggle/input/fintune/content/finetuning_llama3/checkpoint-957 led to unexpected keys not found in the model: score.weight. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device mapping: {'': 'cpu'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint_path = \"/kaggle/input/fintune/content/finetuning_llama3/checkpoint-957\"\n",
    "\n",
    "# Load tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True)\n",
    "\n",
    "# Load model with automatic offloading\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    torch_dtype=\"auto\",  # Automatically choose the best dtype\n",
    "    device_map=\"auto\",  # Automatically distribute layers between CPU & GPU\n",
    "    offload_folder=\"/kaggle/working/offload\",  # Offload large layers to disk\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model device mapping:\", model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T12:11:13.036481Z",
     "iopub.status.busy": "2025-02-26T12:11:13.036105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt, max_length=100):\n",
    "    # Get the correct device for the model's first layer\n",
    "    device = next(model.parameters()).device  \n",
    "\n",
    "    # Move input tensors to the same device as the model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"connection with icon icon dear please setup icon per icon engineers please let other details needed thanks lead\"\n",
    "response = generate_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "onnx_model_path = \"/kaggle/working/llama3_finetuned.onnx\"\n",
    "\n",
    "# Dummy input for ONNX conversion (must match LLaMA input format)\n",
    "dummy_input = tokenizer(\"connection with icon icon dear please setup icon per icon engineers please let other details needed thanks lead\", return_tensors=\"pt\")\n",
    "\n",
    "# Convert the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
    "    onnx_model_path,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "    },\n",
    "    opset_version=17  \n",
    ")\n",
    "\n",
    "print(f\"ONNX model saved at: {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6745947,
     "sourceId": 10859709,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
